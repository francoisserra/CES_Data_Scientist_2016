\documentclass{book}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[francais]{babel}
\usepackage{graphicx} 
\usepackage{fancyref}
\usepackage{hyperref}

\title{Clustering de documents numerisés}
\author{\textsc{Youcef} - \textsc{Kacer}}
\date{20 Aout 2016}

\begin{document}
 
\maketitle

\tableofcontents

\frontmatter
\chapter{Introduction}
Ce document présente un cas très intéressant pour le travail d'archivage : le partitionnement d'un document numérisé en illustration et texte.
La multitude de langues et de polices possibles pour les caractères du texte, le contenu et la disposition très variés des illustrations sont 
autant de paramètres qui permettent difficilement d'imaginer une classification supervisé. En effet, le nombre d'exemples nécessaires peut vite devenir important,
leur labélisation fastidieuse.\\
Dans ce cas, l'apport de la classification non supervisée devient très intéressante. D'autant plus que les caractères d'un document donné sont généralement en grand 
nombre, d'un seul alphabet et d'une seule couleur sur fond blanc (exploitation des edges), alors que les images sont elles assez colorées (exploitation du triplet 
RGB des pixels) : nous allons exploiter tout cela afin de réaliser le clustering d'un document en 3 catégories : illustration, texte et fond.


\mainmatter
\chapter{Images exploitées}
\section{Corpus}\label{labelisation}
Nous allons exploiter un total de 101 images prises des archives du site de l'Université de Californie \cite{uci}. 
Elles représentent chacune une page numérisée tirée de magazines et de journaux russophones \cite{dataset} :

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.2]{images/4.jpg}
\includegraphics[scale=0.2]{images/5.jpg}
\includegraphics[scale=0.2]{images/6.jpg}
\end{center}
\caption{Exemples d'images du dataset}
\label{exemple1}
\end{figure}
\clearpage

\section{Vérité-terrain}
A chacune de ces images est associée une image de vérite-terrain, attribuant à chaque pixel du document numérisé l'une des 3 classes parmi : \\
\begin{description} % listes descriptives

\item[$illustration$ :] Le pixel appartient à une illustration, une image, un fond d'image, un logo (en rouge sur les images vérité-terrain)
\item[$texte$ :] Le pixel appartient à un paragraphe de texte, un titre, un commentaire (en bleu sur les images vérité-terrain)
\item[$fond$ :] Le pixel appartient au fond blanc d'origine de la page (en blanc sur les images vérité-terrain).

\end{description}

Voici quelques exemples de documents numérisés et leur vérité terrain \ref{exemple1} \ref{exemple2} \ref{exemple3}.

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.2]{images/1g.jpg}
\includegraphics[scale=0.2]{images/1g_m.jpg}
\end{center}
\caption{image du dataset et sa vérité terrain}
\label{exemple1}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.2]{images/4cw.jpg}
\includegraphics[scale=0.2]{images/4cw_m.jpg}
\end{center}
\caption{image du dataset et sa vérité terrain}
\label{exemple2}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.2]{images/56.jpg}
\includegraphics[scale=0.2]{images/56_m.jpg}
\end{center}
\caption{image du dataset et sa vérité terrain}
\label{exemple3}
\end{figure}
\clearpage

Par la suite, nous allons expliciter différentes manières d'extraire de l'information afin de pouvoir discriminer automatiquement, au sein d'une image, les pixels de classe
$illustration$, de classe $texte$ et de classe $fond$.

\chapter{Extraction d'information}
\section{Seuillage pour le fond}

Une manière simple et rapide d'isoler le fond, est de réaliser un seuillage de l'image passée en niveau de gris.\\
Le seuil d'Otsu cherche le seuil qui sépare l'histogramme des niveaux de gris en deux classes, de sorte que la variance inter-classe soit maximale, la variance intra-
classe minimale \ref{seuillage}.\\

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.2]{images/1g.jpg}
\includegraphics[scale=0.2]{images/1g_binary.jpg}
\end{center}
\caption{seuillage}
\label{seuillage}
\end{figure}

Ainsi, cette première étape permet de trouver les pixels de $fond$. En effet, on attribue une telle classe aux pixels dont le voisinage carré est à 95\% blanc 
dans l'image binaire.

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.2]{images/1g_binary.jpg}
\includegraphics[scale=0.2]{images/1g_seuil.jpg}
\end{center}
\caption{seuillage puis attribution de la classe $fond$ (voisinage de taille 80 pixels)}
\label{seuillage}
\end{figure}

Reste à definir un classe ($texte$ ou $illustration$) aux pixels restants/ 

\section{Extraction de contours pour le texte}

Les contours au voisinage d'un pixel, semblent être une information pertinente pour caractériser les caractères d'un document numérisé.\\
En effet, les caractères sont généralement d'une couleur uniforme sur un fond uniforme cela afin de rendre la lecture aisée, ce qui a la 
particularité de bien faire ressortir les gradients\\
Pour extraire cette information, deux méthodes seront utilisées : l'application d'un filtre Laplacien et le calcul de l'histogramme orienté du gradient 
\cite{Dala05histogramsof}, cela sur un patch carré. 

\section{Extraction de saturation pour les illustrations}

Comme évoqué precedemment, l'extraction d'une caractéristique de couleur est utile pour isoler les pixels d'illustration\\
Utiliser le triple RGB du voisinage d'un pixel n'a peu de sens si on cherche à discriminer ces pixels des pixels de texte (souvent noir).
En effet, le triplet HSV est plus approprié dans ce contexte : la saturation en particulier pour différencier les images couleur du texte gris ou noir.

\chapter{Custering}

A ce stade, nous avons des pixels qui ne sont pas du $fond$ et auxquels nous devons attribuer une classe parmi $texte$ et $illustration$.
Les features extraits au chapître précédents doivent donc être décomposer en deux sous groupes, à cet effet deux algorithmes sont utilisés :
\begin{description} % listes descriptives

\item[$KMeans$ :] cet algorithme permet de créer deux amas compactes.
\item[$NMF$ :] cet algorithme permet de projeter les features des N pixels. Ces features de dimension P seront projetés sur un espace à 2 dimensions. On crééra ensuite
deux amas : un premier pour les points dont la première composante de projection est plus grande que la seconde, un second pour les autres. 
\item[$fond$ :] Le pixel appartient au fond blanc d'origine de la page (en blanc sur les images vérité-terrain).

\end{description}
\section{Extraction de saturation}
\subsection{Définition}
Hue, Saturation, Value ou HSV est un modèle qui représente la couleur (H) en fonction de la saturation (S) en gris et la luminosité (V) (Cf. figure \ref{HSV}).
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{hue.jpg}
\end{center}
\caption{Illustration $HSV$ \cite{wiki:hsv}}
\label{HSV}
\end{figure}
\begin{description}
\item[Teinte (Hue) :] Correspond à la couleur. Elle est définie sous la forme
d'un cercle où les trois couleurs primaires RVB sont
réparties aux angles 0\degre pour le rouge, 120\degre pour le vert
et 240\degre pour le bleu.
\item[Saturation :] C’est le taux de pureté de la couleur, qui varie entre un
maximale (couleur éclatante) et l’achromatisme (couleur
gris).
\item[Valeur :]  C’est la mesure de l’intensité lumineuse de la couleur
qui varie entre le noir et le blanc.
\end{description}

\subsection{Choix de la variable explicative}

Nous sommes partis de l’hypothèse que les images en \og gros plan \fg{} sont plutôt gris et donc une saturation en couleur plus faible que les images en \og large plan \fg{}.\\
La réalisation d’un histogramme sur la moyenne de saturation (Cf. Figure \ref{hue_histo}), portant sur la totalité des images, nous a permis d’identifier deux classes de type Gaussien pour modéliser nos 2 classes
qui sont les images de type \og gros plan \fg{} et les images de type \og large plan \fg{}\\
Nous présentons le code Python d'extraction de saturation et sa classification dans l'annexe \ref{code_python_saturation}
\label{hue_frontiere}
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{hue_histogramme.jpg}
\end{center}
\caption{Histogramme des moyennes de saturation}
\label{hue_histo}
\end{figure}

La moyenne de la saturation globale obtenue est 60.2561640854. 
Nous avons considérés les images dont la saturation moyenne est inférieure à la moyenne globale comme appartenant à la classe \og gros plan \fg{} et les images dont 
la saturation moyenne est supérieure à la moyenne globale comme appartenant à la classe \og large plan \fg{}.

\section{Histogramme orienté du gradient}

Cette méthode consiste à extraire l'information local de contours. Historiquement, sa première utilisation a consisté en 
la détection de piétons \cite{hog}. Pour une image donnée, le vecteur descripteur des \begin{itshape}HOG\end{itshape} est la concaténation
d'histogrammes de l'amplitude du gradient (en fonction de son orientation). Chaque histogramme est pris sur un carré de l'image
appelé \og cellule \fg{}. L'ensemble des cellules quadrille entièrement l'image avec un eventuel recouvrement.\\
Ces histogrammes sont ensuite normalisés en intensité par paquet de cellules adjacentes (appelés \og block \fg{}) afin d'obtenir des descripteurs 
invariants au changement local d'illumination (Cf. figure \ref{hog}).
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{hog.jpg}
\end{center}
\caption{Construction d'un histogramme orienté du gradient \cite{hog2}}
\label{hog}
\end{figure}

On peut espérer que ces descripteurs s'adaptent bien à notre problème. En effet, les epaules des intervenants à gauche 
et à droite de l'image, ainsi que leur visage, sont des contours descriminants pour la classe $G$ (gros plan)
(Cf. figure \ref{hog_classeG}). D'autre part, les gros plans ont un fond uniforme, 
ce qui donnera beaucoup de gradient nul, et donc un descripteur \begin{itshape}sparse\end{itshape} pour la classe $G$ (gros plan).

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.3]{hog_exemple.jpg}
\includegraphics[scale=0.3]{hog_exemple_contour.jpg}
\includegraphics[scale=0.3]{hog_exemple_gradient.jpg}
\includegraphics[scale=0.3]{hog_exemple2.jpg}
\includegraphics[scale=0.3]{hog_exemple2_contour.jpg}
\includegraphics[scale=0.3]{hog_exemple2_gradient.jpg}
\end{center}
\caption{Histogramme orienté du gradient pour les gros plans (classe $G$)}
\label{hog_classeG}
\end{figure}

\section{Reseaux de neurones à convolution pré-entrainé}

Cette méthode consiste à extraire les descripteurs produits par un réseau de neurones à convolution déjà entrainé.
En effet, en récupérant la sortie de l'avant-dernière couche, on obtient la transformation finale haut-niveau de l'image, 
avant la couche de classification.
Cette technique est connue sous le nom de \begin{itshape}Transfer Learning\end{itshape} \cite{DBLP:journals/corr/YosinskiCBL14} 
et a fait ses preuves dans le cas de réseaux de neurones entrainé sur la base 
d'images \begin{itshape}ImageNet\end{itshape} \cite{imagenet_cvpr09}.
Cette base de près de 10 millions d'images, contient une multitude de classes (animaux,ustensiles,humains,...) organisées hiérarchiquement, 
et on peut espérer que les gros plans (classe $G$) correspondent à l'une de ces classes,et de même pour les images plan 
large (classe $L$), de telle sorte qu'on puisse ensuite séparer les descripteurs via un classifieur 
supervisé.

\chapter{Résultats}
\section{Extraction de contours}
Pour expliquer le résultat de l’extraction de contours, voici quelques exemples d'images où les contours ont été tracés avec 
la fonction $\mathrm{cv::drawContours}$.

La figure \ref{contouringG1} montrent deux exemples de contouring sur des images \og gros plan \fg{} (classe $G$), appliqué
avec l'option \begin{itshape}CHAIN\_APPRO\_NONE\end{itshape} (première passe)

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{contouring_exemple1_classeG_passe1.png}
\includegraphics[scale=0.5]{contouring_exemple2_classeG_passe1.png}
\end{center}
\caption{Contouring sur des images de classe $G$ avec l'option CHAIN\_APPRO\_NONE}
\label{contouringG1}
\end{figure}

La figure \ref{contouringL1} montrent deux exemples de contouring sur des images \og large plan \fg{} (classe $L$), appliqué
avec l'option \begin{itshape}CHAIN\_APPRO\_NONE\end{itshape} (première passe)

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{contouring_exemple1_classeL_passe1.png}
\includegraphics[scale=0.5]{contouring_exemple2_classeL_passe1.png}
\end{center}
\caption{Contouring sur des images de classe $L$ avec l'option CHAIN\_APPRO\_NONE}
\label{contouringL1}
\end{figure}

Il y a plus de contours dans les images \og large plan \fg{} car le décor du plateau TV est composé de beaucoup de détails.
Cela permet de bien différencier les gros plans des plans larges mais pour quelques cas le nombre de 
contours ne suffit pas.\\
Pour remédier à cela, nous avons mis en place la seconde passe et dont voici les résultats pour les mêmes 
images : 

La figure \ref{contouringG2} montre deux exemples de contouring sur des images \og gros plan \fg{} (classe $G$), appliqué
avec l'option \begin{itshape}CHAIN\_APPRO\_SIMPLE\end{itshape} (seconde passe).

La figure \ref{contouringL2} montre deux 
exemples de contouring sur des images \og large plan \fg{} (classe $L$), appliqué avec 
l'option \begin{itshape}CHAIN\_APPRO\_SIMPLE\end{itshape}\\


Le nombre de contours à beaucoup augmenté par rapport à la première passe, ce qui va permettre de différencier encore mieux 
les types de plans : en effet, on note ici un très grand nombre de contours dans le décor et sur les personnages, ce qui nous permet de 
mieux discriminer les plans larges.
Le bandeau du bas est plus détaillé en termes de contours que pour la première passe.

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{contouring_exemple1_classeG_passe2.png}
\includegraphics[scale=0.5]{contouring_exemple2_classeG_passe2.png}
\end{center}
\caption{Contouring sur des images de classe $G$ avec l'option CHAIN\_APPRO\_SIMPLE}
\label{contouringG2}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{contouring_exemple1_classeL_passe2.png}
\includegraphics[scale=0.5]{contouring_exemple2_classeL_passe2.png}
\end{center}
\caption{Contouring sur des images de classe $L$ avec l'option CHAIN\_APPRO\_SIMPLE}
\label{contouringL2}
\end{figure}

\section{Extraction de teinte}
En prenant comme frontière de décision, la valeur calculée en \ref{hue_frontiere}, alors on obtient un taux de prédiction correcte de 78.32\%.\\
Par ailleurs, nous avons demandé à différent classifieurs d'effectuer un apprentissage supervisé sur les moyennes de saturation. Pour cela, nous avons divisé les images
en deux sous-ensembles représentant 70\% du total pour l'entrainement, 30\% pour le test.\\
Les resultats de cette classification sont illustrés dans la table \ref{Table_resultats_hue}. On voit que SVM et Random Forest donnent des frontières plus complexes qui permettent
un meilleure score de test (90\%).\\
Les classifieurs utilisés sont ceux de la librairie \begin{itshape}Scikit-Learn\end{itshape}, implémentés dans le script 
\begin{itshape}Python\end{itshape} tiré de \cite{scikit_bench}, que nous avons adaptés.

\begin{table}
\begin{center}
\begin{tabular}{|p{5cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
\begin{bf}classifieur\end{bf}& \begin{bf}score de test\end{bf} & \begin{bf}temps de training (s)\end{bf} & \begin{bf}temps de test (s)\end{bf} \\
\hline
Ridge Classifier & 0.67 & 0.44 & 0.001\\
\hline
Perceptron &  0.652 & 0.005 & 0.0\\
\hline
Passive-Aggressive & 0.344 & 0.005 & 0.0\\
\hline
kNN & 0.779 & 0.002 & 0.003\\
\hline
Random forest & 0.906 & 0.421 & 0.021\\
\hline
linear SVM - L2 penalty & 0.630 & 0.002 & 0.0\\
\hline
SGDClassifier - L2 penalty & 0.655 & 0.003 & 0.0\\
\hline
linear SVM - L1 penalty	& 0.786 & 0.007 & 0.0\\
\hline
SGDClassifier - L1 penalty & 0.656 & 0.005 & 0.0\\
\hline
SGDClassifier - ElasticNet penalty & 0.616 & 0.005 & 0.0\\
\hline
NearestCentroid & 0.525 & 0.001 & 0.0\\
\hline
SVM & 0.902 & 0.215 & 0.062\\
\hline
linear SVM - L1 penalty - dual=False & 0.670 & 0.067 & 0.0\\
\hline
\end{tabular}
\end{center}
\caption{Résultats de classification supervisée sur les moyennes de saturation}
\label{Table_resultats_hue}
\end{table}

\section{Histogramme orienté du gradient}
Nous avons extrait les descripteurs \begin{itshape}HOG\end{itshape} pour chacune des images de classe $G$ et $L$, en utilisant 
une cellule de taille 32x32 (sans recouvrement) de sorte à récupérer les formes des épaules et du visage, et non les details 
plus petits liés au port de lunettes par exemple. Pour la normalisation, nous l'avons effectué par groupe de carré à 4 cellules (avec
recouvrement) afin de tenir compte du fait qu'au sein des gros plans, les intervenants peuvent être légèrement décalés.
Puis, nous avons appliquer à ces descripteurs, différent classifieurs supervisés. 
Nous avons découpé le set de descripteurs en deux sous-ensembles représentant 70\% du total pour l'entrainement, 
30\% pour le test. Les resultats de classification sont illustrés dans la table \ref{Table_resultats_hog}.\\
Les classifieurs utilisés sont ceux de la librairie \begin{itshape}Scikit-Learn\end{itshape}, implémentés dans le script 
\begin{itshape}Python\end{itshape} tiré de \cite{scikit_bench}, que nous avons adaptés.

\begin{table}
\begin{center}
\begin{tabular}{|p{5cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
\begin{bf}classifieur\end{bf}& \begin{bf}score de test\end{bf} & \begin{bf}temps de training (s)\end{bf} & \begin{bf}temps de test (s)\end{bf} \\
\hline
Ridge Classifier & 1.0 & 0.130 & 0.027\\
\hline
Perceptron & 1.0 & 0.219 & 0.028\\
\hline
Passive-Aggressive & 1.0 & 0.352 & 0.024\\
\hline
kNN & 0.998 & 0.221 & 2.875\\
\hline
Random forest & 1.0 & 1.321 & 0.012\\
\hline
linear SVM - L2 penalty & 1.0 & 0.197 & 0.024\\
\hline
SGDClassifier - L2 penalty & 1.0 & 0.202 & 0.024\\
\hline
linear SVM - L1 penalty & 1.0 & 0.283 & 0.024\\
\hline
SGDClassifier - L1 penalty & 1.0 & 0.634 & 0.024\\
\hline
SGDClassifier - ElasticNet penalty & 1.0 & 0.721 & 0.024\\
\hline
NearestCentroid & 0.996 & 0.049 & 0.028\\
\hline
SVM & 0.998 & 0.446 & 0.203\\
\hline
linear SVM - L1 penalty - dual=False & 1.0 & 0.238 & 0.003\\
\hline
\end{tabular}
\end{center}
\caption{Résultats de classification supervisée sur les descripteurs $HOG$}
\label{Table_resultats_hog}
\end{table}

\section{Reseaux de neurones à convolution pré-entrainé}
Nous avons utilisé le code \begin{itshape}Overfeat\end{itshape} \cite{DBLP:journals/corr/SermanetEZMFL13}, récupéré par clonage du dépôt 
Github correspondant \cite{overfeat}, pré-entrainé sur la base \begin{itshape}ImageNet\end{itshape} \cite{imagenet_cvpr09}.
Nous avons extrait les descripteurs pour chacune des images de classe $G$ et $L$ pour leur appliquer différent classifieurs
 supervisés. Nous avons découpé le set de descripteurs en deux sous-ensembles 70\% du total pour l'entrainement, 30\% pour le test.
 Les resultats de classification sont illustrés dans la table \ref{Table_resultats_overfeat}.\\
 Les classifieurs utilisés sont ceux de la librairie \begin{itshape}Scikit-Learn\end{itshape}, implémentés dans le script 
\begin{itshape}Python\end{itshape} tiré de \cite{scikit_bench}, que nous avons adaptés.

 
\begin{table}
\begin{center}
\begin{tabular}{|p{5cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
\begin{bf}classifieur\end{bf}& \begin{bf}score de test\end{bf} & \begin{bf}temps de training (s)\end{bf} & \begin{bf}temps de test (s)\end{bf} \\
\hline
Ridge Classifier & 1.0 & 0.762 & 0.152\\
\hline
Perceptron & 0.998 & 1.515 & 0.155\\
\hline
Passive-Aggressive & 1.0 & 1.852 & 0.150\\
\hline
kNN & 0.998 & 1.456 & 16.77\\
\hline
Random forest & 0.998 & 3.851 & 0.024\\
\hline
linear SVM - L2 penalty & 1.0 & 1.105 & 0.149\\
\hline
SGDClassifier - L2 penalty & 0.998 & 1.156 & 0.153\\
\hline
linear SVM - L1 penalty & 1.0 & 1.220 & 0.149\\
\hline
SGDClassifier - L1 penalty & 1.0 & 5.581 & 0.149\\
\hline
SGDClassifier - ElasticNet penalty & 1.0 & 6.062 & 0.151\\
\hline
NearestCentroid	& 0.987 & 0.302 & 0.221\\
\hline
SVM & 1.0 & 3.508 & 1.591\\
\hline
linear SVM - L1 penalty - dual=False & 1.0 & 1.150 & 0.011\\
\hline
\end{tabular}
\end{center}
\caption{Résultats de classification supervisée sur les descripteurs $Overfeat$}
\label{Table_resultats_overfeat}
\end{table}
\clearpage

\appendix
\chapter{Code Python pour l'extraction de saturation et sa classification}\label{code_python_saturation}
\begin{verbatim}
import numpy as np
import cv2
import os
import matplotlib.pyplot as plt
import shutil
import pandas as pd

saturation = []
path='c:/tmp/images_debat/'
dirs = os.listdir(path)
file_valid='c:/tmp/labels.csv'
# This would print all the files and directories
for file in dirs:
    if file.endswith((".jpg")):
        colorImage = cv2.imread(path+file)
        #print ('Traitement du fichier : ',path+file)
        rows, cols, nbChannels = colorImage.shape

        b,g,r = cv2.split(colorImage)
        colorImage=cv2.merge([r,g,b])
        color = ('b','g','r')
        clorImageHSV=cv2.cvtColor(colorImage, cv2.COLOR_BGR2HSV)
        h,s,v=cv2.split(clorImageHSV)
        sm=np.mean(s)
        saturation.append(sm)
        #if file == '00000108.jpg' or file == '00000110.jpg' or file == '00000147.jpg':
            #print ('saturation pour le fichier 108 - 110 -147:')
            #print sm

globalMean = np.mean(saturation)
print ('Moyenne globale:')
print globalMean
#print saturation
plt.hist(saturation, range = (35, 85), bins = 30, color = 'yellow',
            edgecolor = 'red')
plt.xlabel('Moyenne de la saturation')
plt.ylabel('nombre images')
plt.title('Histogramme saturation')
#hist=np.histogram(saturation)
#plt.bar(hist)
plt.show()
copyPathGrosPlan='c:/tmp/GrosPlan/'
copyPathLargePlan='c:/tmp/LargePlan/'
classif=pd.DataFrame(columns=('file_name', 'label'))
classif_multi=pd.DataFrame(columns=('file_name', 'sm'))
line = 0;
for file1 in dirs:
    if file1.endswith((".jpg")):
        colorImage = cv2.imread(path+file1)
        rows, cols, nbChannels = colorImage.shape

        r,g,b = cv2.split(colorImage)
        colorImage=cv2.merge([r,g,b])
        color = ('b','g','r')
        clorImageHSV=cv2.cvtColor(colorImage, cv2.COLOR_BGR2HSV)
        h,s,v=cv2.split(clorImageHSV)
        sm=np.mean(s)
        classif_multi.loc[line]=(file1,sm)
        #Gros plan
        if  sm <= (globalMean):
             classif.loc[line]=(file1,-1.)            
        #Large plan
        else:
            classif.loc[line]=(file1,1.)            
        
        line = line+1

classif.sort_values(['file_name'])
#print classif            
#Open the labelled file
labelfile = open(file_valid)
label_img = pd.read_csv(labelfile,sep=';', names=['file_name', 'label'],header=0,dtype={'label': float})
label_img.sort_values(['file_name'])
#print label_img

correct_predict = pd.merge(classif, label_img, on=['file_name', 'label'], how='inner')
#print len(correct_predict)
#print len(label_img)
print ('Resultat :', (float(len(correct_predict))/float(len(label_img)))*100.)
#print classif_multi
#print label_img
label_img_mrg = pd.merge(classif_multi, label_img, on=['file_name'], how='inner')
label_img_mrg = label_img_mrg.drop('file_name', 1)
#print label_img_mrg
label_img_mrg.to_csv('C:\perso\TP\Son-Image\classif_multi.csv', sep=';', encoding='utf-8')
\end{verbatim}

\backmatter

\listoftables

\listoffigures

\bibliographystyle{alpha}
\bibliography{biblio}

\end{document}